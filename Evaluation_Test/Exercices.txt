1. Scenario 1:
You have been given salary details of employees in a file 'salarydet.txt' in the folder 'sample-files-for-test'.

This file contains
id:employee_id
name:employee_name
bonus:bonus_percentage
Payment:yop:year_of_payment & current_salary:current_salary

Note**
--File has data delimited by delimiters
--Payment column supports 'map' datatype


sample contents of salarydet.txt
----------
10,smith,10,1styear:210000
20,peter,10,1styear:410000


Tasks to Do:
Load this data into a dir 'salary' on hdfs
Create a MANAGED hive table 'salarydata1m' which can hold this data.The table should be in default location of hive.
Load data into table using load statement from hdfs
Check if file exists in your 'salary' directory
Create another EXTERNAL table 'salarydata2m' pointing to your MANAGED table location

Query the table MANAGED/EXTERNAL table to find these:


--show current_salary as salary along with other columns where year_of_payment value is 1styear.
--show current_salary as salary along with other columns where year_of_payment value is 1styear and order by salary in desc order
  --check if there were any null values and analyze why
  --check if MapReduce job was trigger and see how much time it took
--use function to round off the value of total salary after appling bonus percentage to salary where year_of_payment is 1styear and
display this data in desc order
--find total sum paid as salary (salar+bonus) where year_of_payment is 1styear
--drop your external table

Using Pyspark,query the managed table:
--test the same queries from PySpark instead of hive
--check if it triggers any MapReduce
--save output of any query in a json file on hdfs
--save output of any query in a default format on hdfs(parquet)

--check files in hdfs to confirm if data was written
--if multiple files were written in hdfs, investigate why?
--write the code to have data written only in one file

#Optional
-- read data from the file (salarydet.txt) in HDFS location using PySpark(Dataframes/RDD)
   --This location could be your hive table folder
   --This location could be your hdfs directory
--filter the data where year_of_payment is not 1st year and save this output in a CSV file on hdfs

Scenario 2:

You have been given sentiments data in a file 'cv000_29416.txt' in the folder 'sample-files-for-test'.

Tasks to do:
Load this data in hdfs by creating a directory 'sentiments'
--write code in scala/python to perform wordcount on this file and then
    --find how many words were found by getting a count
    --show the output of this wordcount code
    --print only 10 entries from this result
    --save the output on hdfs in folder 'result' > check files on hdfs
    --save the output in one file on hdfs if multiple files were created in previous save operation

Scenario 3:
You have been given bankdata in a file 'Bank_full.csv' in the folder 'sample-files-for-test'.

Tasks to Do:
Using Hive:

--Load data into hdfs without creating a specific directory on hdfs, i.e. directly on hdfs '/'
--Create hive table 'bankdata' to hold this data and then load this data into table from your local filesystem ie from 'sample-files-for-test' or wherever this file was downloaded
--get count of rows in table
--Load the data into hive table without overwriting using the same file. Your table should have the data 3 times.
--get count of rows in the table
--Run queries to:


Using Spark>PySpark/Spark-Shell
--Load the data from 'Bank_full.csv' file on hdfs into a Spark dataframe
--Write code to Run Queries to:

--You can also load data from 'Bank_full.csv' file on hdfs into a Spark RDD
--Write code to work on RDD:


