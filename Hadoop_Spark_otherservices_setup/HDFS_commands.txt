ls /etc/h*
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/conf
[ajaykuma24gmail@ip-10-0-42-218 ~]$ ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop/bin
fuse_dfs  hadoop
[ajaykuma24gmail@ip-10-0-42-218 ~]$ ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop-hdfs/bin
hdfs
[ajaykuma24gmail@ip-10-0-42-218 ~]$ ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop-yarn/bin
container-executor  mapred  yarn
[ajaykuma24gmail@ip-10-0-42-218 ~]$ ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop-mapreduce/bin
mapred  pipes-sort  wordcount-nopipe  wordcount-part  wordcount-simple
[ajaykuma24gmail@ip-10-0-42-218 ~]$ 
[ajaykuma24gmail@ip-10-0-42-218 ~]$ #hadoop
[ajaykuma24gmail@ip-10-0-42-218 ~]$ #hdfs
[ajaykuma24gmail@ip-10-0-42-218 ~]$ #mapred
[ajaykuma24gmail@ip-10-0-42-218 ~]$ #yarn

[ajaykuma24gmail@ip-10-0-42-218 ~]$ hadoop version
Hadoop 3.0.0-cdh6.3.2
Source code repository http://github.com/cloudera/hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89
Compiled by jenkins on 2019-11-08T13:49Z
Compiled with protoc 2.5.0
From source with checksum f539c87da37534aad732f2a7ddcc59
This command was run using /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/hadoop-common-3.0.0-cdh6.3.2.jar

hdfs dfs -mkdir mydata3
hdfs dfs -ls /user/ajaykuma24gmail
hdfs dfs -ls -R /user/ajaykuma24gmail
hdfs dfs -copyFromLocal Bank_full.csv /user/ajaykuma24gmail/mydata3
hdfs dfs -copyFromLocal *.csv /user/ajaykuma24gmail
hdfs dfs -copyFromLocal -f *.csv /user/ajaykuma24gmail
hdfs dfs -rm -R Bank_full.csv
hdfs dfs -ls -R /user/ajaykuma24gmail/.Trash
hdfs dfs -rm -R -skipTrash Cardiotocographic.csv
hdfs dfs -put Cardiotocographic.csv mydata
hdfs fsck /user/ajaykuma24gmail -blocks
hdfs fsck /user/ajaykuma24gmail -blocks -files
hdfs dfs -D dfs.replication=2 -put *.csv mydata4
hdfs dfs -mkdir mydata5
hdfs dfs -D dfs.replication=4 -put *.log mydata5
hdfs fsck /user/ajaykuma24gmail/mydata5 -blocks -files
hdfs dfs -setrep -R -w 3 mydata5
hdfs fsck /user/ajaykuma24gmail/mydata5 -blocks -files
hdfs dfs -du
hdfs dfs -du -h
hdfs dfs -df -h 
hdfs dfs -count -q /user/ajaykuma24gmail
hdfs dfsadmin -setQuota 5000 /user/ajaykuma24gmail (only with admin access possible)
hdfs dfsadmin -setSpaceQuota 25g /user/ajaykuma24gmail (only with admin access possible)
hdfs dfs -put -f txt_sentoken mydata
hdfs dfs -put -f txt_sentoken newdata (this command will fail due to quota limit)
hdfs dfs -rm -R -skipTrash newdata
hdfs dfs -rm -R -skipTrash mydata
hdfs dfs -cp mydata3 mydata2
hdfs dfs -mv mydata3/Bank_full.csv mydata5
cat /etc/hadoop/conf/core-site.xml | grep hdfs

    <value>hdfs://nameservice1</value>

hadoop distcp hdfs://nameservice1/user/ajaykuma24gmail/mydata hdfs://nameservice1/user/ajaykuma24gmail/mydatab
hdfs dfs -help count
====================================
Important configs to look for:
hdfs-site.xml
--repl--dfs.replication
--metadata path for nn
--data path for dn
--data path for snn
--http
--https
--blocksize--dfs.blocksize
core-size.xml
--fs.defaultFS
yarn-site.xml
mapred-site.xml
hadoop-env.sh
yarn-env.sh
capacity-scheduler.xml







