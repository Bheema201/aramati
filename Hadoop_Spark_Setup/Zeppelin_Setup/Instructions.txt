#Setup Zeppelin with Hadoop and Spark Integrated
------------------------------------------

Refer this link to setup spark on windows
https://github.com/ajaykuma/BigData_HESS/blob/main/Spark_n_Scala/SetupSparkonWindows.txt

once done..

download zeppelin from https://zeppelin.apache.org/download.html (look in old releases section)

zeppelin-0.8.2-bin-all.tgz

--unpack the downloaded file
--go into I:\zeppelin-0.8.2-bin-all\conf folder and change the template files 
shiro.ini.template -- shiro.ini
zeppelin-env.cmd.template -- zeppelin-env.cmd ( windows command script file)
zeppelin-env.sh.template -- zeppelin-env.sh ( Shell script)
zeppelin-site.xml.template -- zeppelin-site.xml (xml document)

--update the following in zeppelin-env.sh file ( to point to hadoop and spark)
export JAVA_HOME=C:\Java\jdk1.8.0_221
export SPARK_HOME=C:\spark-2.4.3-bin-hadoop2.6
export HADOOP_CONF_DIR=C:\Users\Win10\Desktop\hadoop

--check in shiro.ini and update/set password for admin user

--go into I:\zeppelin-0.8.2-bin-all\bin and run
zeppelin (windows command) [preferably as administrator]
--Look for 
 INFO [2021-02-16 08:17:12,257] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5ad851c9{zeppelin-web,/,file:///I:/zeppelin-0.8.2-bin-all/webapps/webapp/,AVAILABLE}{I:\zeppelin-0.8.2-bin-all\zeppelin-web-0.8.2.war}
 INFO [2021-02-16 08:17:12,287] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@149b0577{HTTP/1.1,[http/1.1]}{127.0.0.1:8080}
 INFO [2021-02-16 08:17:12,288] ({main} Server.java[doStart]:407) - Started @37102ms
 INFO [2021-02-16 08:17:12,289] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started


access zeppelin
http://127.0.0.1:8080

#Testing zeppelin

< from right top menu > interpreter
--search for spark
master: local[*]    
#this can later be changed to yarn-cluster to integrate zeppelin with spark & hadoop's yarn
spark.app.name: zeppelin
#this can be customized
zeppelin.pyspark.python:python
#points to python of your machine

--create a notebook

#using spark with scala
	#using RDD API
val x = sc.parallelize(1 to 10)
val y = x.map(n => n * 100)
val z = y.filter(n=> n%50 == 0)
z.collect

	#using dataframe API
val x = spark.read.json("I:\\GitContent\\Datasets\\Datasets\\primer-dataset2.json")
x.printSchema()
x.show(10)
x.select("name","cuisine").show(10)
x.filter($"cuisine" === "Hamburgers").show(5)

#using spark with python









