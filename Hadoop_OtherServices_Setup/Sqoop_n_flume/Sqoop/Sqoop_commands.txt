ls /opt/cloudera/parcels/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/sqoop/bin--in CDH
ls /usr/local/sqoop --in apache cluster

sqoop help
sqoop list-databases --connect jdbc:mysql://c1/hive?useSSL=false --username hdu -P
Abcd@1234

sqoop list-databases --connect jdbc:mysql://c1 --username hdu -P
sqoop import --connect jdbc:mysql://c1/mysql --username hdu -P --table help_keyword

sqoop import --connect jdbc:mysql://c1/mysql --username hdu -P --table help_keyword --m 1 --target-
dir /user/hdu/helpk1

sqoop import --connect jdbc:mysql://c1/mysql --username hdu -P --table help_keyword --m 6 --target-dir /user/hdu/helpk2

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --m 6 --target-dir /user/hdu/helpk3

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --m 1 --target-dir /user/hdu/helpk2 -z

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --m 1 --target-dir /user/hdu/helpk3 --compression-codec 
org.apache.hadoop.io.compress.SnappyCodec

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --m 1 --target-dir /user/hdu/helpk4 --compression-codec 
org.apache.hadoop.io.compress.BZip2Codec

cat /etc/hadoop/conf/core-site.xml | grep compress

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --m 1 --target-dir /user/hdu/helpk5 --compression-codec org.apache.hadoop.io.compress.Lz4Codec

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --m 1 --target-dir /user/hdu/helpk6 --compression-codec org.apache.hadoop.io.compress.DefaultCodec 

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql 
--username hdu -P -
-m 1 --target-dir /user/hdu/helpk9 --query 
"select help_keyword_id from help_keyword 
where help_keyword_id > 300 and \$CONDITIONS"

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-m 1 --table help_keyword --target-dir /user/hdu/helpk10 --as-sequencefile

sqoop import "-Dmapreduce.framework.name=local" "-Dmapreduce.output.fileoutputformat.compress=true" "-Dmapred.
output.compression.type=RECORD" --connect jdbc:mysql://c1/mysql --username hdu -P --m 1 --table 
help_keyword --target-dir /user/hdu/helpk12 --as-sequencefile

sqoop import "-Dmapreduce.framework.name=local" "-Dmapreduce.output.fileoutputformat.compress=true" "-Dmapred.
output.compression.type=BLOCK" --connect jdbc:mysql://c1/mysql --username hdu -P --m 1 --table help_keyword --target-dir /user/hdu/helpk11 --as-sequencefile

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-m 1 --table help_keyword --target-dir /user/hdu/helpk13 --as-avrodatafile

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-m 1 --table help_keyword --target-dir /user/hdu/helpk14 --as-parquetfile ****** to check

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/testaj --username hdu -P 
--table testajtbl2 --target-dir /user/hdu/helpk15 --split-by help_keyword_id

sqoop import "-Dmapreduce.framework.name=local" "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" --conne
ct jdbc:mysql://c1/testaj --username hdu -P --table testajtbl2 --target-dir /user/hdu/helpk17 
--split-by name
----------------------------
importing into hbase and testing..

sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --target-dir /user/hdu/helpk19 --hbase-create-table --hbase-table helpnewhbtbl --column-family impinfo

hbase shell
scan 'helpnewhbtbl'
get 'helpnewhbtbl', '98'

-----------------------
importing into hive and testing...
sqoop import "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/mysql --username hdu -P -
-table help_keyword --target-dir /user/hdu/helpk18 --hive-import --create-hive-table --hive-table helpnewhvtbl


hive
select * from helpnewhvtbl limit 10;
describe extended helpnewhvtbl; 
select * from helpnewhvtbl where help_keyword_id > 600 order by help_keyword_id desc;

export into mysql
sqoop export "-Dmapreduce.framework.name=local" --connect jdbc:mysql://c1/testaj 
--username hdu -P 
--table testajtbl3 --export-dir /user/hdu/helpk1
============================
Output from HDFS
[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk1                                                                
Found 2 items
-rw-r--r--   2 hdu hdu          0 2018-10-07 06:05 /user/hdu/helpk1/_SUCCESS
-rw-r--r--   2 hdu hdu       8264 2018-10-07 06:05 /user/hdu/helpk1/part-m-00000
[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk2                                                                
Found 2 items
-rw-r--r--   2 hdu hdu          0 2018-10-07 06:08 /user/hdu/helpk2/_SUCCESS
-rw-r--r--   2 hdu hdu       4035 2018-10-07 06:08 /user/hdu/helpk2/part-m-00000.gz
[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk3                                                                
Found 2 items
-rw-r--r--   2 hdu hdu          0 2018-10-07 06:10 /user/hdu/helpk3/_SUCCESS
-rw-r--r--   2 hdu hdu       6373 2018-10-07 06:10 /user/hdu/helpk3/part-m-00000.snappy
[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk4                                                                
Found 2 items
-rw-r--r--   2 hdu hdu          0 2018-10-07 06:12 /user/hdu/helpk4/_SUCCESS
-rw-r--r--   2 hdu hdu       3407 2018-10-07 06:12 /user/hdu/helpk4/part-m-00000.bz2
[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk5                                                                
Found 2 items
-rw-r--r--   2 hdu hdu          0 2018-10-07 06:16 /user/hdu/helpk5/_SUCCESS
-rw-r--r--   2 hdu hdu       6304 2018-10-07 06:16 /user/hdu/helpk5/part-m-00000.lz4

[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk8                                                                
Found 2 items
-rw-r--r--   2 hdu hdu          0 2018-10-07 06:20 /user/hdu/helpk8/_SUCCESS
-rw-r--r--   2 hdu hdu       4023 2018-10-07 06:20 /user/hdu/helpk8/part-m-00000.deflate

[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk10                                                               
Found 2 items
-rw-r--r--   2 hdu hdu          0 2018-10-07 06:35 /user/hdu/helpk10/_SUCCESS
-rw-r--r--   2 hdu hdu      22265 2018-10-07 06:35 /user/hdu/helpk10/part-m-00000
[hdu@ip-10-0-1-10 ~]$ hdfs dfs -ls /user/hdu/helpk9 


