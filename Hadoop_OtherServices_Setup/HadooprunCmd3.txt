exam aligned Format questions:
1.files given > asking to read-process-write
(file forats: variety)
2. complete the code

x = sc.textFile("mydata.txt",4)

?
result = 4

3. writing a code snippet to achieve something...
--reading data
--processing data
--processing data in a specific way
--writing data back..

4. incorrect code > correct code

5.

================
Working with HDFS
hdfs dfs -mkdir /mydata --- will fail because of permission issue

hdfs dfs -mkdir mydata
hdfs dfs -ls mydata
hdfs dfs -rm -R mydata
2021-08-02 03:38:16,391 INFO fs.TrashPolicyDefault: Moved: 'hdfs://m01.itversity.com:9000/user/itv000966/mydata' 
to trash at: hdfs://m01.itversity.com:9000/user/itv000966/.Trash/Current/user/itv000966/mydata

hdfs dfs -ls /user/itv000966
hdfs dfs -ls /user/itv000966/.Trash/Current/user/itv000966

#copy
hdfs dfs -cp /user/itv000966/.Trash/Current/user/itv000966/mydata /user/itv000966/

hdfs dfs -ls /user/itv000966/.Trash/Current/user/itv000966
hdfs dfs -ls /user/itv000966/

#restore
hdfs dfs -mv /user/itv000966/.Trash/Current/user/itv000966/mydata /user/itv000966/mydatarest

#for configs
ls /opt/hadoop/etc/hadoop/
ls /etc/h*
more /etc/hadoop/conf/core-site.xml | grep trash
more /etc/hadoop/conf/hdfs-site.xml | grep replication
more /etc/hadoop/conf/hdfs-site.xml

hdfs dfs -ls 

hdfs dfs -rm -R -skipTrash mydata
hdfs dfs -rm -R -skipTrash mydatarest

hdfs dfs -ls -R /user/hive

get files from datasets folder
abc1.txt
Bank_full.csv
employees.json
kv1.txt
people.json
users.avro
users.parquet
recipes.json

and from bigdata repo>hive folder>
salarydet.txt
hivef4.txt
hivef3.txt
hivef5.txt

hdfs dfs -mkdir mydata
hdfs dfs -mkdir bankdata
hdfs dfs -mkdir usersdata

hdfs dfs -copyFromLocal mydata-local1/ mydata
hdfs dfs -ls -R mydata
hdfs dfs -put mydata-local1/Bank_full.csv bankdata
hdfs dfs -ls -R bankdata
hdfs dfs -put mydata-local1/users* usersdata

hdfs dfs -copyToLocal mydata .
hdfs dfs -get mydata/mydata-local1 mydata-local2
hdfs dfs -put -f mydata-local1 mydata

more /etc/hadoop/conf/core-site.xml
get hdfs path > 

hdfs://m01.itversity.com:9000

hdfs dfs -ls hdfs://m01.itversity.com:9000/

hadoop distcp hdfs://m01.itversity.com:9000/user/itv000966/mydata/mydata-local1 hdfs://m01.itversity.com:9000/user/itv000966/mydata2


vi test.txt
hdfs dfs -put test.txt mydata/mydata-local1
hdfs dfs -ls mydata/mydata-local1

hadoop distcp -update hdfs://m01.itversity.com:9000/user/itv000966/mydata/mydata-local1 hdfs://m01.itversity.com:9000/user/itv000966/mydata2
hadoop distcp -overwrite hdfs://m01.itversity.com:9000/user/itv000966/mydata/mydata-local1 hdfs://m01.itversity.com:9000/user/itv000966/mydata2

hadoop distcp -overwrite hdfs://m01.itversity.com:9000/user/itv000966/mydata/mydata-local1/Bank_full.csv hdfs://m01.itversity.com:9000/user/itv000966/Bank_full2.csv

hadoop distcp -Dmapreduce.framework.name=local -overwrite hdfs://m01.itversity.com:9000/user/itv000966/mydata/mydata-local1/Bank_full.csv hdfs://m01.itversity.com:9000/user/itv000966/mydata2

https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
https://hadoop.apache.org/docs/r2.7.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml

hadoop distcp -Dmapreduce.framework.name=local -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec -overwrite hdfs://m01.itversity.com:9000/user/itv000966/mydata/mydata-local1/Bank_full.csv hdfs://m01.itversity.com:9000/user/itv000966/mydata2
hadoop distcp -Dmapreduce.output.fileoutputformat.compress=True -Dmapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec hdfs://m01.itversity.com:9000/user/itv000966/mydata/mydata-local1/Bank_full.csv hdfs://m01.itversity.com:9000/user/itv000966/mydata2

















