#working in cdh cluster (irrespective of your version)/Apache hadoop core cluster

#in cdh cluster

ls /etc/h*
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/conf
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop/bin
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop-hdfs/bin
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop-yarn/bin
ls /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop-mapreduce/bin
--similarly we can look at binaries (ie commands) for other services in lib folder

#we will use these commands and their options to work with Hadoop from CLI
#to use commands , use them without '#'
#these commands work in cdh/apache core from CLI

#hadoop
#hdfs
#mapred
#yarn
#hive

--in CDH
$ hadoop version
Hadoop 3.0.0-cdh6.3.2
Source code repository http://github.com/cloudera/hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89
Compiled by jenkins on 2019-11-08T13:49Z
Compiled with protoc 2.5.0
From source with checksum f539c87da37534aad732f2a7ddcc59
This command was run using /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/hadoop-common-3.0.0-cdh6.3.2.jar

#if logged in as 'hdfs'
hdfs dfs -mkdir mydata3
(hdfs dfs -mkdir /mydata3)[if user has permissions to write directly to hdfs]
hdfs dfs -ls /user/hdfs
hdfs dfs -ls -R /user/hdfs
hdfs dfs -copyFromLocal Bank_full.csv /user/hdfs/mydata3
hdfs dfs -copyFromLocal *.csv /user/hdfs
hdfs dfs -copyFromLocal -f *.csv /user/hdfs
hdfs dfs -rm -R Bank_full.csv
hdfs dfs -ls -R /user/hdfs/.Trash
hdfs dfs -rm -R -skipTrash Cardiotocographic.csv
hdfs dfs -put Cardiotocographic.csv mydata
hdfs fsck /user/hdfs -blocks
hdfs fsck /user/hdfs -blocks -files
hdfs dfs -D dfs.replication=2 -put *.csv mydata4
hdfs dfs -mkdir mydata5
hdfs dfs -D dfs.replication=4 -put *.log mydata5
hdfs fsck /user/hdfs/mydata5 -blocks -files
hdfs dfs -setrep -R -w 3 mydata5
hdfs fsck /user/ajaykuma24gmail/mydata5 -blocks -files
hdfs dfs -du
hdfs dfs -du -h
hdfs dfs -df -h 
hdfs dfs -count -q /user/ajaykuma24gmail
hdfs dfsadmin -setQuota 5000 /user/ajaykuma24gmail (only with admin access possible)
hdfs dfsadmin -setSpaceQuota 25g /user/ajaykuma24gmail (only with admin access possible)
hdfs dfs -put -f txt_sentoken mydata
hdfs dfs -put -f txt_sentoken newdata (this command will fail due to quota limit)
hdfs dfs -rm -R -skipTrash newdata
hdfs dfs -rm -R -skipTrash mydata
hdfs dfs -cp mydata3 mydata2
hdfs dfs -mv mydata3/Bank_full.csv mydata5

#looking for hdfs path
cat /etc/hadoop/conf/core-site.xml | grep hdfs

<value>hdfs://nameservice1</value>

#if path shows above , its a High Availability cluster..

hadoop distcp hdfs://nameservice1/user/hdfs/mydata hdfs://nameservice1/user/hdfs/mydatab
hdfs dfs -help count
====================================
Important configs to look for:

hdfs-site.xml
--for replication
--metadata path for nn 
--data path for dn
--data path for snn
--http
--https
--blocksize

core-size.xml
--for hdfs path
  fs.defaultFS

yarn-site.xml
--for resourcemanager path
mapred-site.xml
hadoop-env.sh
yarn-env.sh
capacity-scheduler.xml

Other commands:
Test Hadoop Cluster :
to test write performance,write 10 files each of 1000 megabytes.
It will write an output in a dir /benchmarks where all metrics are written.
$bin/hadoop jar hadoop-test-0.20.205.0.jar TestDFSIO -write -nrFiles 10 -fileSize 1000
to test read performance,read 10 files each of 1000 megabytes.It will write an output in a dir /benchmarks where all metrics are written.
$bin/hadoop jar hadoop-test-0.20.205.0.jar TestDFSIO -read -nrFiles 10 -fileSize 1000
$bin/hadoop jar hadoop-test-0.20.205.0.jar TestDFSIO -clean

Generate Tera Data:
to generate dummy data within hdfs instead of downloading datset from somewhere nd uploading on hdfs.
$bin/hadoop jar hadoop/hadoop-examples...jar teragen 1000 /user/hduser/terasort-input
using your dummy data you can check sorting performance
$bin/hadoop jar hadoop/hadoop-examples...jar terasort /user/hduser/terasort-input /user/hduser/terasort-output
$bin/hadoop job -history all /user/hduser/terasort-input

===========================================================








